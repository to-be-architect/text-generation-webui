WizardLM_WizardCoder-15B-V1.0$:
  loader: Transformers
  cpu_memory: 0
  auto_devices: true
  disk: true
  cpu: false
  bf16: false
  load_in_8bit: false
  trust_remote_code: true
  load_in_4bit: false
  compute_dtype: float16
  quant_type: nf4
  use_double_quant: false
  wbits: None
  groupsize: None
  model_type: llama
  pre_layer: 0
  triton: false
  desc_act: false
  no_inject_fused_attention: false
  no_inject_fused_mlp: false
  no_use_cuda_fp16: false
  threads: 0
  n_batch: 512
  no_mmap: false
  mlock: false
  n_gpu_layers: 0
  n_ctx: 2048
  llama_cpp_seed: 0.0
  gpu_split: ''
  max_seq_len: 2048
  compress_pos_emb: 1
  gpu_memory_0: 20000
  gpu_memory_1: 20000
